<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machine | 知行录]]></title>
  <link href="http://whbzju.github.com/blog/categories/machine/atom.xml" rel="self"/>
  <link href="http://whbzju.github.com/"/>
  <updated>2015-01-02T13:40:34+08:00</updated>
  <id>http://whbzju.github.com/</id>
  <author>
    <name><![CDATA[阿波]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[机器学习基石课程总结]]></title>
    <link href="http://whbzju.github.com/blog/2014/12/28/about-ml-fundation-course/"/>
    <updated>2014-12-28T11:07:00+08:00</updated>
    <id>http://whbzju.github.com/blog/2014/12/28/about-ml-fundation-course</id>
    <content type="html"><![CDATA[<p>课程一开始，提了四个topic，what every machine learning user should know</p>

<pre><code>* when can ml learn
* why can ml learn
* how can ml learn
* how can ml learn better
</code></pre>

<h2 id="when-can-ml-learn">When can ml learn</h2>
<p>首先，机器学习针对的场景，通过<em>A</em>对<em>D</em>和<em>H</em>学习一个g，用来描述最终的目标f，而这个事情无法简单的用规则搞定。其次，澄清各类细分ml场景的定义：</p>

<pre><code>* 监督式
* 非监督式
* 增强学习
* 推进系统
* Activity学习，通过asking来学习
* Streaming
</code></pre>

<h2 id="why-can-ml-learn">why can ml learn</h2>
<pre><code>* shatter的概念
* break point的概念
* generation问题
* VC维的概念
</code></pre>

<h2 id="how-can-ml-learn">how can ml learn</h2>
<p>讲了一些基本的linear方法，比如logistic regression，顺便提了下nonlinear的问题，通过transform将nonlinear映射到linear可分的空间，有点类似核函数，需要进一步确认。</p>

<h2 id="how-can-ml-learn-better">how can ml learn better</h2>
<pre><code>* overfiting
* regularition，这块数学不错。从拉格朗日的constraint说起，到L1和L2的直观意义。
* cv
* 三个重要的Principle。Occam's Razor， Sample Bias， Data Snooping.
</code></pre>

<h2 id="section">我对这门课的收获</h2>

<pre><code>* 霍夫曼定理
* VC维的理解，线性相关
* 略微有点啰嗦，为了避免数学，导致描述的复杂度上升好几个级别
</code></pre>

<p>总体来讲，对工程人员帮助不是特别大，但有利于加深概念的理解。</p>

<to be="" continue="">
</to>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ipython notebook对机器学习工程师的价值]]></title>
    <link href="http://whbzju.github.com/blog/2014/11/23/ipython-notebook-bring-to-me/"/>
    <updated>2014-11-23T16:17:00+08:00</updated>
    <id>http://whbzju.github.com/blog/2014/11/23/ipython-notebook-bring-to-me</id>
    <content type="html"><![CDATA[<p><strong>关键词</strong>：代码、数据、文档合一。</p>

<hr />

<p>如果选一个关键词来描述机器学习工程师的工作，不断试错是我心中的number one。相对于软件工程师来讲，有大量琐碎的dirty需要做，通常会占据到80%左右的时间。一个好的工具能够极大的提高效率。</p>

<p>总结需求如下：</p>

<ol>
  <li>可交互式的环境：比如预处理数据，有的时候数据比较大，比较耗时，希望能处理一次后就放在内存里面使用。</li>
  <li>文档化，记录工作流。数据挖掘会有非常多的idea要去尝试，实现这些idea的代码会有微小的差异，需要一个工具能够统一追踪管理他们。且不同的实验会有不同的结果，整理这些结果形成文档太费时间，希望能够做完实验就生成文档。</li>
  <li>经常会有一些片段代码要写，写在文件里有太零碎，写在交互式的shell里面有很难回溯，需要一个交互式和文档结合的工具。</li>
  <li>支持可视化工具，兼容python画图</li>
</ol>

<h2 id="ipython-notebook">神奇的ipython notebook</h2>

<h3 id="section">安装环境</h3>
<p>非常简单，推荐：Anaconda, <a href="https://store.continuum.io/cshop/anaconda/">官网</a></p>

<h3 id="ipython-notebook-1">ipython notebook入门</h3>
<p>还是<a href="http://ipython.org/notebook.html">官网</a>, 一开始不适应的同学，多看几个example吧。</p>

<h3 id="ipython-notebook-2">分享你的ipython notebook</h3>
<p>一键分享：<a href="http://nbviewer.ipython.org/">A simple way to share Jupyter Notebooks</a></p>

<h2 id="section-1">最后</h2>
<p>附一张我的在kaggle上用ipython notebook做的一个入门题照：
<img src="http://wujiarawsrc.qiniudn.com/sklearn-kaggle.png" alt="剧照" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[聚类算法中常见的距离计算方法]]></title>
    <link href="http://whbzju.github.com/blog/2014/11/22/ml-distance-measure/"/>
    <updated>2014-11-22T22:01:00+08:00</updated>
    <id>http://whbzju.github.com/blog/2014/11/22/ml-distance-measure</id>
    <content type="html"><![CDATA[<h2 id="section">概述</h2>
<p>在面对聚类问题时，选择何种距离计算方法求相似度是一个basic question。文献[1]中提到了N多计算方法，从大类来看有以下几种：</p>

<ul>
  <li>$L_p$ Minkowski家族</li>
  <li>$ L_1 $ 家族</li>
  <li>Intersection 家族</li>
  <li>Inner Product家族</li>
  <li>etl
简单算一下大概有40+个计算方法，其中有好多没有听过。好在工业界一般只涉及到几个，本文将按自己理解大致介绍下这些方法及应用情况。</li>
</ul>

<h2 id="section-1">距离的类型和尺度</h2>
<p>类型：</p>

<ul>
  <li>二进制（binary）</li>
  <li>离散值（Discrete）</li>
  <li>连续值(Continuous)</li>
</ul>

<p>尺度：</p>

<ul>
  <li>定性：比如同义：red、green、black，比如顺序：高、中、低</li>
  <li>定量：
  a) interval
  b) ratio
距离的类型和尺度非常重要，影响后续聚类算法的选择。</li>
</ul>

<h2 id="section-2">距离计算方法定义</h2>
<p>严谨的定义参考[4]，通俗来讲，在一个空间内，距离计算方法满足以下4个公理。</p>

<ol>
  <li>$d(x,y) ≥ 0$</li>
  <li>$ d(x,y)=0$ if $x=y$</li>
  <li>$ d(x, y) = d(y, x)$ (distance is symmetric)</li>
  <li>$d(x, y) ≤ d(x, z) + d(z, y)$ (the triangle inequality).</li>
</ol>

<p>在欧式空间，第四个公理可以直观理解为两点之间距离最短。在其他情况需要一些证明才能推导。</p>

<h2 id="section-3">常见的距离计算方法</h2>

<h3 id="lr-norm">Lr norm</h3>
<p>在n维空间，其计算公式如下：</p>

<p>$d(x,y)=(\sum_{k=1}^{n} |x_k-y_k| ^r)^{1/r}$</p>

<ul>
  <li>
    <p>欧式距离
当r=2，这就是我们熟悉的欧式距离，其聚类形状在二维空间是一个圆。归属于$L_2$ norm</p>
  </li>
  <li>
    <p>曼哈顿距离
r=1，归属于$L_1$ norm，其名字的来源与该距离计算过程有关。该距离类似在x和y的每个维度上沿grid line上travel，类似曼哈顿的街道。</p>
  </li>
  <li>
    <p>$L_∞$ norm
当r不断变大，该式中只有max $|x_i-y_i|$项起作用，故又称L_max norm</p>
  </li>
</ul>

<h3 id="jaccard-distance">Jaccard Distance</h3>
<p>类似于相识度，x和y每个维度上相同的值的个数/总的维度。</p>

<p>The comparison of two binary vectors, p and q, leads to four quantities: </p>

<ul>
  <li>M01 = the number of positions where p was 0 and q was 1 </li>
  <li>M10 = the number of positions where p was 1 and q was 0 </li>
  <li>M00 = the number of positions where p was 0 and q was 0 </li>
  <li>M11 = the number of positions where p was 1 and q was 1 </li>
</ul>

<p>The simplest similarity coefficient is the simple matching coefficient </p>

<p>$ J = (M11) / (M01 + M10 + M11) $ </p>

<p>如：</p>

<ul>
  <li>a = 1 0 0 0 0 0 0 0 0 0</li>
  <li>b = 0 0 0 0 0 0 1 0 0 1
J = 0</li>
</ul>

<h3 id="cosine-distance">cosine Distance</h3>
<p>即余弦公式。求x和y两个特征向量的夹角。
cos( d1, d2 ) = (d1 • d2) / ||d1|| ||d2|| </p>

<h3 id="edit-distance">Edit Distance</h3>
<p>一般适用于String point。通过不断的删除和添加单个字符来计算两个string之间的距离。</p>

<h3 id="hamming-distance">Hamming Distance</h3>
<p>pass</p>

<h2 id="section-4">参考</h2>

<ul>
  <li>[1] Comprehensive survey on distance/similarity measures between probability density functions. SH Cha
City, 2007 - csis.pace.edu</li>
  <li>[2] An Introduction to Cluster Analysis for Data Mining</li>
  <li>[3] Unsupervised and Semi-supervised Clustering:
a Brief Survey</li>
  <li>[4] Mining of Massive Datasets - chapter 3</li>
</ul>
]]></content>
  </entry>
  
</feed>
