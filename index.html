
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>知行录</title>
  <meta name="author" content="阿波">

  
  <meta name="description" content="问题 最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://whbzju.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="知行录" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" -->
<!--link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" -->
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-16146431-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">知行录</a></h1>
  
    <h2>rethinking</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:whbzju.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">博客主页</a></li>
  <li><a href="/blog/archives">文章列表</a></li>
  <li><a href="/about">关于</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/10/feature-hashing/">机器学习技巧之feature_hashing</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-02-10T15:58:00+08:00" pubdate data-updated="true">Feb 10<span>th</span>, 2015</time>
        
         | <a href="/blog/2015/02/10/feature-hashing/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">问题</h2>

<p>最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3&gt;1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。</p>

<p>再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。
维基上的定义：</p>

<blockquote>
  <p>In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array</p>
</blockquote>

<h2 id="section-1">解决方案</h2>

<p>维基上关于Feature Hash的描述非常清晰，各位自己去看，不再累赘，这里多说一点hash的方法。常见的有以下两种实现：</p>

<pre><code>function hashing_vectorizer(features : array of string, N : integer):
 x := new vector[N]
 for f in features:
     h := hash(f)
     x[h mod N] += 1
 return x
</code></pre>

<p>另外还有一种：</p>

<pre><code>function hashing_vectorizer(features : array of string, N : integer):
 x := new vector[N]
 for f in features:
     h := hash(f)
     idx := h mod N
     if ξ(f) == 1:
         x[idx] += 1
     else:
         x[idx] -= 1
 return x
</code></pre>

<p>可以理解，既然是hash，必然要付出collision时的代价。实现方案一并没有考虑处理冲突，N越长，冲突的概率越低，然后存储的要求会变大。实现二，通过有符号的hash来解决冲突问题，即有很大概率在出现冲突时，该hash值为0，即不起作用，更详细的描述参考文献2.</p>

<h2 id="sklearn-featurehasher">sklearn FeatureHasher的实现</h2>

<p><code>class sklearn.feature_extraction.FeatureHasher(n_features=1048576, input_type='dict', dtype=&lt;type 'numpy.float64'&gt;, non_negative=False)</code>，该接口返回一个sparse类型的array。</p>

<blockquote>
  <p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
</blockquote>

<p>该接口需要注意的是数据入参，支持三种格式：pair、dict和string。可以参考官方的<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/tests/test_feature_hasher.py">featurehasher test</a>。</p>

<p>stackoverflow上也有一个比较好的例子：</p>

<p>Q:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class=""><span class="line">I am using FeatureHasher in scikit-learn.
</span><span class="line">
</span><span class="line">Can anyone explain why I end up with 4 non zero data in the sparse matrix instead of 2 after the following:
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; f = FeatureHasher(input_type='string')
</span><span class="line">&gt;&gt;&gt; g = f.transform(('as','bs'))
</span><span class="line">&lt;2x1048576 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
</span><span class="line">with 4 stored elements in Compressed Sparse Row format&gt;
</span><span class="line">&gt;&gt;&gt; g=f.transform(('as','bs'))
</span><span class="line">&gt;&gt;&gt; g.data
</span><span class="line">array([-1.,  1., -1., -1.])
</span><span class="line">&gt;&gt;&gt; g.nonzero()
</span><span class="line">(array([0, 0, 1, 1], dtype=int32), array([341263, 354738,  98813, 341263], dtype=int32))
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>A:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class=""><span class="line">It appears is expecting a sequence of sequences. The outer sequence being for the observations, and the inner being features. With your input, the inner sequence are the characters of the string.
</span><span class="line">
</span><span class="line">Observation 0: 'a' -&gt; 354738, 's' -&gt; 341263
</span><span class="line">
</span><span class="line">Observation 1: 'b' -&gt; 98813, 's' -&gt; 341263
</span><span class="line">
</span><span class="line">Try this:
</span><span class="line">
</span><span class="line">g = f.transform([['as'],['bs']])
</span><span class="line">For output:
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; g.nonzero()
</span><span class="line">(array([0, 1], dtype=int32), array([494108, 335425], dtype=int32))</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="section-2">参考文献</h2>

<p>[1] <a href="http://en.wikipedia.org/wiki/Feature_hashing">Feature hashing From Wikipedia, the free encyclopedia</a></p>

<p>[2] <a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.</a></p>

<p>[3] <a href="http://scikit-learn.org/stable/modules/feature_extraction.html">sklearn feature hashing</a></p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/07/git-practices/">Git 实践</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-02-07T08:54:00+08:00" pubdate data-updated="true">Feb 7<span>th</span>, 2015</time>
        
         | <a href="/blog/2015/02/07/git-practices/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="git-rebasegit-branch">git rebase和git branch</h2>
<p>svn过来的同学一定会觉得git的分支管理好方便，但更应该了解的是git rebase。可以说，用不用git rebase是区分你熟不熟悉git的重要方式。</p>

<h2 id="git-reset">git reset</h2>

<h2 id="git-fetchgit-pull">git fetch和git pull</h2>

<h2 id="git-commit--amend">git commit -amend</h2>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/02/2014-readlist/">2014读过的书和参加的公开课</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-02T10:45:00+08:00" pubdate data-updated="true">Jan 2<span>nd</span>, 2015</time>
        
         | <a href="/blog/2015/01/02/2014-readlist/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">技术类</h2>

<h3 id="section-1">机器学习</h3>

<ol>
  <li>Pattern Recognizition and Machine Learning. 目前完成前四章，还没有完全吸收，希望2015的成长能够顺利吸收这本书的知识。</li>
  <li>推荐系统cookbook。感觉这本书有些落后于时代，大致翻了下。</li>
  <li>推荐系统实践–项亮。入门好读物。</li>
  <li>Frontiers in Massive Data Analysis。综述型，推荐。</li>
  <li>The Elements of Statistical Learning : Data Mining, Inference, andPrediction。当做工具书，还没有读多少，先把PRML搞定。</li>
  <li>kaggle solution分享。</li>
</ol>

<h3 id="python">Python</h3>

<ol>
  <li>python cookbook. 非常推荐。</li>
  <li>廖雪峰的python<a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000">教程</a>. 简洁概要，实战内容有难度，很适合提供自己编程水平。</li>
  <li>python for data analysis。了解到pandas，数据分析利器。</li>
  <li>sklearn官方文档。图文并茂，极力推荐。</li>
  <li>pandas官方文档。例子丰富，入门先推荐10min那篇。</li>
</ol>

<h3 id="r">R</h3>
<p>粗略看了写文档。</p>

<h2 id="section-2">非技术类</h2>

<ol>
  <li>文明之光（上下），推荐，吴军博士的书质量一如既往。</li>
  <li>女士品茶。概率论发展史及大牛八卦。</li>
  <li>英语语法俱乐部–施元佑。大力推荐，介绍语法的来龙去脉。以前太不看重语法，阅读和写作的瓶颈。</li>
  <li>金字塔原理。</li>
  <li>思考的艺术</li>
  <li>如何阅读一本书</li>
  <li>数理统计简史</li>
</ol>

<h2 id="section-3">公开课</h2>

<ol>
  <li>Andrew Ng的Machine Learing。完成</li>
  <li>台大的机器学习基石。完成</li>
  <li>Functional Programming Principles in Scala。继续上</li>
  <li>Mining Massive Datasets，正在上</li>
  <li>機器學習技法 (Machine Learning Techniques)。正在上</li>
</ol>

<h2 id="section-4">互联网资讯</h2>

<ol>
  <li>咨询：依旧是知乎，目前没有看到什么能替代。</li>
  <li>kaggle，非常好的机器学习学习平台</li>
  <li>cousera，2015能学更多有价值的课程。</li>
</ol>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/28/about-ml-fundation-course/">机器学习基石课程总结</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-12-28T11:07:00+08:00" pubdate data-updated="true">Dec 28<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/12/28/about-ml-fundation-course/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>课程一开始，提了四个topic，what every machine learning user should know</p>

<pre><code>* when can ml learn
* why can ml learn
* how can ml learn
* how can ml learn better
</code></pre>

<h2 id="when-can-ml-learn">When can ml learn</h2>
<p>首先，机器学习针对的场景，通过<em>A</em>对<em>D</em>和<em>H</em>学习一个g，用来描述最终的目标f，而这个事情无法简单的用规则搞定。其次，澄清各类细分ml场景的定义：</p>

<pre><code>* 监督式
* 非监督式
* 增强学习
* 推进系统
* Activity学习，通过asking来学习
* Streaming
</code></pre>

<h2 id="why-can-ml-learn">why can ml learn</h2>
<pre><code>* shatter的概念
* break point的概念
* generation问题
* VC维的概念
</code></pre>

<h2 id="how-can-ml-learn">how can ml learn</h2>
<p>讲了一些基本的linear方法，比如logistic regression，顺便提了下nonlinear的问题，通过transform将nonlinear映射到linear可分的空间，有点类似核函数，需要进一步确认。</p>

<h2 id="how-can-ml-learn-better">how can ml learn better</h2>
<pre><code>* overfiting
* regularition，这块数学不错。从拉格朗日的constraint说起，到L1和L2的直观意义。
* cv
* 三个重要的Principle。Occam's Razor， Sample Bias， Data Snooping.
</code></pre>

<h2 id="section">我对这门课的收获</h2>

<pre><code>* 霍夫曼定理
* VC维的理解，线性相关
* 略微有点啰嗦，为了避免数学，导致描述的复杂度上升好几个级别
</code></pre>

<p>总体来讲，对工程人员帮助不是特别大，但有利于加深概念的理解。</p>

<to be="" continue="">
</to>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/30/r-language-conference/">记第七届R语言大会</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-30T11:01:00+08:00" pubdate data-updated="true">Nov 30<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/11/30/r-language-conference/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>这届R语言大会在杭师仓前校区举行，由阿里巴巴承办。邀请到了libsvm的作者林老师。但据同事说林教授演讲用的ppt都是一套（汗~~~）。上午是主会场，有四位嘉宾做了介绍，其他几位介绍的比较范，林稍微带点干货，特别提到数据没有到20T，不一定要上big data。下午是分会场，本人去听了：</p>

<ul>
  <li>分析师使用的R包</li>
  <li>京东的ctr模型</li>
  <li>天猫learning to rank</li>
  <li>天猫数据驱动运营。</li>
</ul>

<h2 id="section">点评</h2>

<ul>
  <li>京东分享推荐的ctr模型。和我街处于同一起跑线，模型和特征做法都很相似。赞下京东的分享着，讲的很实在。</li>
  <li>天猫l2r。没什么干货，介绍了一些基本概念就结束了。</li>
  <li>天猫数据驱动运营。预测销量，c2b。介绍的比较范，而且个人觉得他讲的效果有夸大的嫌疑。</li>
  <li>分析师用的R包。R中类似ipython notebook的东西。可交互的图，非常实用。</li>
</ul>

<h2 id="section-1">感想</h2>

<ul>
  <li>Spark出现的频率很高。</li>
  <li>用R来作云服务，我个人觉得不靠谱。</li>
  <li>libsvm的作者好有趣。演讲前看论文，演讲后睡觉。工业界范十足。</li>
  <li>学术界的分享依旧不靠谱。</li>
  <li>IBM关于电信领域的挖掘是来搞笑的吗</li>
  <li>接下来就等ppt了。</li>
</ul>

<p>最后，和阿里比，其他公司的所处的阶段还是非常的初级。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/23/ipython-notebook-bring-to-me/">Ipython Notebook对机器学习工程师的价值</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-23T16:17:00+08:00" pubdate data-updated="true">Nov 23<span>rd</span>, 2014</time>
        
         | <a href="/blog/2014/11/23/ipython-notebook-bring-to-me/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>关键词</strong>：代码、数据、文档合一。</p>

<hr />

<p>如果选一个关键词来描述机器学习工程师的工作，不断试错是我心中的number one。相对于软件工程师来讲，有大量琐碎的dirty需要做，通常会占据到80%左右的时间。一个好的工具能够极大的提高效率。</p>

<p>总结需求如下：</p>

<ol>
  <li>可交互式的环境：比如预处理数据，有的时候数据比较大，比较耗时，希望能处理一次后就放在内存里面使用。</li>
  <li>文档化，记录工作流。数据挖掘会有非常多的idea要去尝试，实现这些idea的代码会有微小的差异，需要一个工具能够统一追踪管理他们。且不同的实验会有不同的结果，整理这些结果形成文档太费时间，希望能够做完实验就生成文档。</li>
  <li>经常会有一些片段代码要写，写在文件里有太零碎，写在交互式的shell里面有很难回溯，需要一个交互式和文档结合的工具。</li>
  <li>支持可视化工具，兼容python画图</li>
</ol>

<h2 id="ipython-notebook">神奇的ipython notebook</h2>

<h3 id="section">安装环境</h3>
<p>非常简单，推荐：Anaconda, <a href="https://store.continuum.io/cshop/anaconda/">官网</a></p>

<h3 id="ipython-notebook-1">ipython notebook入门</h3>
<p>还是<a href="http://ipython.org/notebook.html">官网</a>, 一开始不适应的同学，多看几个example吧。</p>

<h3 id="ipython-notebook-2">分享你的ipython notebook</h3>
<p>一键分享：<a href="http://nbviewer.ipython.org/">A simple way to share Jupyter Notebooks</a></p>

<h2 id="section-1">最后</h2>
<p>附一张我的在kaggle上用ipython notebook做的一个入门题照：
<img src="http://wujiarawsrc.qiniudn.com/sklearn-kaggle.png" alt="剧照" /></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/22/ml-distance-measure/">聚类算法中常见的距离计算方法</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-22T22:01:00+08:00" pubdate data-updated="true">Nov 22<span>nd</span>, 2014</time>
        
         | <a href="/blog/2014/11/22/ml-distance-measure/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">概述</h2>
<p>在面对聚类问题时，选择何种距离计算方法求相似度是一个basic question。文献[1]中提到了N多计算方法，从大类来看有以下几种：</p>

<ul>
  <li>$L_p$ Minkowski家族</li>
  <li>$ L_1 $ 家族</li>
  <li>Intersection 家族</li>
  <li>Inner Product家族</li>
  <li>etl
简单算一下大概有40+个计算方法，其中有好多没有听过。好在工业界一般只涉及到几个，本文将按自己理解大致介绍下这些方法及应用情况。</li>
</ul>

<h2 id="section-1">距离的类型和尺度</h2>
<p>类型：</p>

<ul>
  <li>二进制（binary）</li>
  <li>离散值（Discrete）</li>
  <li>连续值(Continuous)</li>
</ul>

<p>尺度：</p>

<ul>
  <li>定性：比如同义：red、green、black，比如顺序：高、中、低</li>
  <li>定量：
  a) interval
  b) ratio
距离的类型和尺度非常重要，影响后续聚类算法的选择。</li>
</ul>

<h2 id="section-2">距离计算方法定义</h2>
<p>严谨的定义参考[4]，通俗来讲，在一个空间内，距离计算方法满足以下4个公理。</p>

<ol>
  <li>$d(x,y) ≥ 0$</li>
  <li>$ d(x,y)=0$ if $x=y$</li>
  <li>$ d(x, y) = d(y, x)$ (distance is symmetric)</li>
  <li>$d(x, y) ≤ d(x, z) + d(z, y)$ (the triangle inequality).</li>
</ol>

<p>在欧式空间，第四个公理可以直观理解为两点之间距离最短。在其他情况需要一些证明才能推导。</p>

<h2 id="section-3">常见的距离计算方法</h2>

<h3 id="lr-norm">Lr norm</h3>
<p>在n维空间，其计算公式如下：</p>

<p>$d(x,y)=(\sum_{k=1}^{n} |x_k-y_k| ^r)^{1/r}$</p>

<ul>
  <li>
    <p>欧式距离
当r=2，这就是我们熟悉的欧式距离，其聚类形状在二维空间是一个圆。归属于$L_2$ norm</p>
  </li>
  <li>
    <p>曼哈顿距离
r=1，归属于$L_1$ norm，其名字的来源与该距离计算过程有关。该距离类似在x和y的每个维度上沿grid line上travel，类似曼哈顿的街道。</p>
  </li>
  <li>
    <p>$L_∞$ norm
当r不断变大，该式中只有max $|x_i-y_i|$项起作用，故又称L_max norm</p>
  </li>
</ul>

<h3 id="jaccard-distance">Jaccard Distance</h3>
<p>类似于相识度，x和y每个维度上相同的值的个数/总的维度。</p>

<p>The comparison of two binary vectors, p and q, leads to four quantities: </p>

<ul>
  <li>M01 = the number of positions where p was 0 and q was 1 </li>
  <li>M10 = the number of positions where p was 1 and q was 0 </li>
  <li>M00 = the number of positions where p was 0 and q was 0 </li>
  <li>M11 = the number of positions where p was 1 and q was 1 </li>
</ul>

<p>The simplest similarity coefficient is the simple matching coefficient </p>

<p>$ J = (M11) / (M01 + M10 + M11) $ </p>

<p>如：</p>

<ul>
  <li>a = 1 0 0 0 0 0 0 0 0 0</li>
  <li>b = 0 0 0 0 0 0 1 0 0 1
J = 0</li>
</ul>

<h3 id="cosine-distance">cosine Distance</h3>
<p>即余弦公式。求x和y两个特征向量的夹角。
cos( d1, d2 ) = (d1 • d2) / ||d1|| ||d2|| </p>

<h3 id="edit-distance">Edit Distance</h3>
<p>一般适用于String point。通过不断的删除和添加单个字符来计算两个string之间的距离。</p>

<h3 id="hamming-distance">Hamming Distance</h3>
<p>pass</p>

<h2 id="section-4">参考</h2>

<ul>
  <li>[1] Comprehensive survey on distance/similarity measures between probability density functions. SH Cha
City, 2007 - csis.pace.edu</li>
  <li>[2] An Introduction to Cluster Analysis for Data Mining</li>
  <li>[3] Unsupervised and Semi-supervised Clustering:
a Brief Survey</li>
  <li>[4] Mining of Massive Datasets - chapter 3</li>
</ul>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/13/machine-learning-at-coursera/">Coursera上Andrew Ng的机器学习课程评价</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-13T21:49:00+08:00" pubdate data-updated="true">Sep 13<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/09/13/machine-learning-at-coursera/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">前言</h2>
<p>该课程非常适合初学者，比原先Ng在斯坦福大学的公开课要简单许多，少去了许多数学，并且在课程内容安排上更加紧凑，涉及的内容广泛，旨在搭建初学者与机器学习的桥梁。值得一提的是，该课程较为接近工程的角度，故我建议有兴趣的工程师可以尝试下。</p>

<p>本人花了3周左右的时间完成了该课程的视频和project，并拿到证书。每天下班回来比较晚，倒腾一两个小时，就该洗洗睡了，还好妹子理解。但不建议初学者像我一样，原因如下：</p>

<ul>
  <li>本人有一定的机器学习基础。</li>
  <li>虽然不熟悉octave，但是写过几年程序，能较快的适应它的语法。</li>
  <li>该课程没有中文字幕，不过Andrew Ng将的英文都比较简单，大家可以放心，我一般加速1.5x听。</li>
</ul>

<p>该课程有个较为合理的时间预估，可根据它安排学习计划。</p>

<hr />

<h2 id="section-1">内容介绍</h2>

<h3 id="section-2">建议先看视频</h3>

<p>Ng的视频和ppt做的非常有质量，简洁易懂。该课程有19个课程，每个课程一般有4-6个小视频，视频时间有长有短，最长的不会超过20分钟。下面列下个人觉得最有价值的部分：</p>

<ul>
  <li>Octave Tutorial，该教程能够帮你快速了解octave的基本用法，后续所有的project都会涉及到它。请对照该课程的讲义熟悉octave。PS：mac上安装octave可能会出些问题，我这边只要取消安装text help部分即可。</li>
  <li>主流算法，Logistic回归，SVM，ANN，Kmeans，PCA，协同过滤等，可以理解为通俗易懂版的讲解，不满足的同学建议去看看Ng在斯坦福公开课中的视频，那边会有较详细的数学推导。</li>
  <li>最为精华，即本人认为该课程最核心的部分是：X. Advice for Applying Machine Learning (Week 6)和XVIII. Application Example: Photo OCR。因为其他知识很容易得到，而且可以得到更好更完备的解释，而这两个视频，是Ng从工程的角度指导我们如何应用机器学习，并且给了一个实际的例子。</li>
</ul>

<h3 id="project">建议完成所有课程的project</h3>
<p>由于这些project的说明文档写的非常仔细，较为容易完成。通过写一些代码，可以对这些算法有更深的理解。另外，也可以认识到octave vectorizing写法的威力。</p>

<p>PS：请不要着急去网上搜课程答案，Ng在课程里面明确说明不要把作业答案放到网上，但还是有不少人放了自己的答案。提前看了答案，你会少很多乐趣。</p>

<hr />

<h2 id="section-3">总结</h2>
<p>后来也看了几个coursera上的课程，暂时还么有找到能和它一拚的课程。如果有哪位同学知道，可以分享下。
当然，这个课程只是个开始，后续可以看斯坦福大学的机器学习课程，同时配合Pattern Recognized and Machine Learning这本书效果会更好。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/08/markdown-gramma/">Markdown_gramma(转)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-08T22:46:00+08:00" pubdate data-updated="true">Apr 8<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/04/08/markdown-gramma/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>个人收藏用，转：« <a href="http://wowubuntu.com">访问 Wow!Ubuntu</a></p>

<p><strong>NOTE:</strong> This is Simplelified  Chinese Edition Document of Markdown Syntax. If you are seeking for English Edition Document. Please refer to <a href="http://daringfireball.net/projects/markdown/syntax">Markdown: Syntax</a>.</p>

<p><strong>声明：</strong> 这份文档派生(fork)于<a href="http://markdown.tw/">繁体中文版</a>，在此基础上进行了繁体转简体工作，并进行了适当的润色。此文档用 Markdown 语法编写，你可以到这里<a href="http://gitcafe.com/riku/Markdown-Syntax-CN/blob/master/syntax.md">查看它的源文件</a>。「繁体中文版的原始文件可以<a href="https://github.com/othree/markdown-syntax-zhtw/blob/master/syntax.md">查看这里</a> 。」–By @<a href="http://twitter.com/riku">riku</a></p>

<p><strong>注：</strong> 本项目托管于 <a href="http://gitcafe.com/riku/Markdown-Syntax-CN/">GitCafe</a>上，请通过”派生”和”合并请求”来帮忙改进本项目。</p>

<h1 id="markdown----basichtml">Markdown 语法说明 (简体中文版) / (<a href="./basic.html">点击查看快速入门</a>)</h1>

<ul>
  <li><a href="#overview">概述</a>
    <ul>
      <li><a href="#philosophy">宗旨</a></li>
      <li><a href="#html">兼容 HTML</a></li>
      <li><a href="#autoescape">特殊字符自动转换</a></li>
    </ul>
  </li>
  <li><a href="#block">区块元素</a>
    <ul>
      <li><a href="#p">段落和换行</a></li>
      <li><a href="#header">标题</a></li>
      <li><a href="#blockquote">区块引用</a></li>
      <li><a href="#list">列表</a></li>
      <li><a href="#precode">代码区块</a></li>
      <li><a href="#hr">分隔线</a></li>
    </ul>
  </li>
  <li><a href="#span">区段元素</a>
    <ul>
      <li><a href="#link">链接</a></li>
      <li><a href="#em">强调</a></li>
      <li><a href="#code">代码</a></li>
      <li><a href="#img">图片</a></li>
    </ul>
  </li>
  <li><a href="#misc">其它</a>
    <ul>
      <li><a href="#backslash">反斜杠</a></li>
      <li><a href="#autolink">自动链接</a></li>
    </ul>
  </li>
  <li><a href="#acknowledgement">感谢</a></li>
  <li><a href="#editor">Markdown 免费编辑器</a></li>
</ul>

<hr />

<h2 id="overview">概述</h2>

<h3 id="philosophy">宗旨</h3>

<p>Markdown 的目标是实现「易读易写」。
</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2014/04/08/markdown-gramma/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/07/windows-tips/">Windows Tips</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-07T10:00:00+08:00" pubdate data-updated="true">Apr 7<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/04/07/windows-tips/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content">
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>ZJU CS小硕 爱读书 爱代码 相信移动互联网</p>
</section>

<section>
  <h1>新浪微博</h1>
  <ul id="weibo">
    <li>
      <iframe 
        width="100%" 
        height="550" 
        class="share_self" 
        frameborder="0" 
        scrolling="no" 
        src="http://widget.weibo.com/weiboshow/index.php?width=0&height=550&ptype=1&speed=0&skin=&isTitle=0&noborder=1&isWeibo=1&isFans=&uid=1911447995&verifier=4e804b1d">
      </iframe>
    </li>
  </ul>
</section>

<section>
	<h1>Categories</h1>
    	<ul id="categories">
	        <li><a href='/blog/categories/c'>C</a></li><li><a href='/blog/categories/coding'>coding</a></li><li><a href='/blog/categories/git'>git</a></li><li><a href='/blog/categories/jni'>JNI</a></li><li><a href='/blog/categories/learning'>learning</a></li><li><a href='/blog/categories/mac'>Mac</a></li><li><a href='/blog/categories/machine'>machine</a></li><li><a href='/blog/categories/maching'>maching</a></li><li><a href='/blog/categories/octopress'>octopress</a></li><li><a href='/blog/categories/os'>OS</a></li><li><a href='/blog/categories/thoughts'>Thoughts</a></li><li><a href='/blog/categories/tryos'>TryOS</a></li><li><a href='/blog/categories/vim'>vim</a></li><li><a href='/blog/categories/公开课'>公开课</a></li><li><a href='/blog/categories/计步器'>计步器</a></li><li><a href='/blog/categories/读书'>读书</a></li>
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/02/10/feature-hashing/">机器学习技巧之feature_hashing</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/07/git-practices/">git 实践</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/01/02/2014-readlist/">2014读过的书和参加的公开课</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/12/28/about-ml-fundation-course/">机器学习基石课程总结</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/30/r-language-conference/">记第七届R语言大会</a>
      </li>
    
  </ul>
</section>






  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

<p>
  Copyright &copy; 2015 - 阿波 -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'hbwu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
