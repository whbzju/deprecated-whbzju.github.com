
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>知行录</title>
  <meta name="author" content="阿波">

  
  <meta name="description" content="现有爆发的互联网模式，体现在将原先只有一小部分人能享受到的服务面向大众，从而实现对现有产业的改造。比如小米，原先苹果三星的价格导致大部分用户无法享受他们的产品。小米提供了一个相对优质的产品，牺牲了不少品质，比如外观、做工，但成功将价格定在大众可接受的区间，从而实现了爆发式的增长。这种例子， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://whbzju.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="知行录" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<!--link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" -->
<!--link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" -->
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-16146431-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">知行录</a></h1>
  
    <h2>rethinking</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:whbzju.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">博客主页</a></li>
  <li><a href="/blog/archives">文章列表</a></li>
  <li><a href="/about">关于</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/04/18/think-about-weidian-bussiness/">微店引发的互联网模式思考</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-04-18T20:48:00+08:00" pubdate data-updated="true">Apr 18<span>th</span>, 2015</time>
        
         | <a href="/blog/2015/04/18/think-about-weidian-bussiness/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>现有爆发的互联网模式，体现在将原先只有一小部分人能享受到的服务面向大众，从而实现对现有产业的改造。比如小米，原先苹果三星的价格导致大部分用户无法享受他们的产品。小米提供了一个相对优质的产品，牺牲了不少品质，比如外观、做工，但成功将价格定在大众可接受的区间，从而实现了爆发式的增长。这种例子，传统行业也有过很多例子，比如优衣库、Zara，将上流用户的衣服，通过降低一定的质量和设计，完成价格革命，让普通用户享受到时尚，从而实现服装行业的革命。相同的例子还有福特，将汽车面向普通大众。这些例子说明一个，在品质和价格间做好balance，将优质的服务面向大众，具有极大的潜力，同时也说明了一个行业趋向成熟。</p>

<p>回过头来仔细想下，为何苹果三星不直接面向大众提供服务。这里面除了品牌定位，还涉及到一个行业的发展。在行业发展初期，各种成本都是高昂的，所以只能面向一些高端用户。高端用户的定义可以是：对价格不敏感，但对品质有较高的要求。够用和追求品质是区分普通商品和奢侈品的重要区别，俗话说一分钱一分货，十分钱两分货。奢侈品对于人类有重要的意义，在这些no functional的地方持续的投入，恰是文明的来源。拉回来看前面的例子，若要将高端用户使用的服务面向大众，其品质必然会有所下降，但会进入一个处于可接受的范围，接着极力的压榨价格区间，进入市场后照成破坏性的增长，一家独霸。这里小米做到了，淘宝也做到了一半。</p>

<p>来看今天的主题—微店，即微商。上面说到淘宝做到了一半，即对传统的零售商业模式的冲击并没有完成一家独霸，彻底改造。其原因在于，其商品价格下来后，优质的服务不能保证，即没有进入一个合理的区间。这里面的问题比较复杂，刨去假货不说，很多时候商品质量不好，真就是商家无法提供优质的服务。因为这些优质的服务被传统大佬们垄断着。淘宝可以抹除店面、分销的成本，但无法解决生产的问题，当这些大佬进入电商后，很快就被击垮。那为什么要提到微商，因为中国作为一个世界大工厂已经很多年，其生产能力在不断的提升，但缺少产品设计和分销渠道。而微商类似于，拥有这些资源的个人，不愿意被传统的渠道垄断，但有无法承担建立品牌的费用，借力与人与人直接的信任传播来实现分销。</p>

<p>这里重点讲下品牌，淘宝最大的问题还是在于流量越来越贵，小商户直接就出局了，没有办法顺利的进入成长期。而传统的大佬在建立品牌投入的巨大成本，需要靠一定的利润空间收回。所以，如果想打赢这场仗，必须抹除建立品牌的成本，而这也是微商可能能够提供的。天下没有免费得晚餐，如果不是某个环节的效率提高了导致价格的下降，这种模式是不可持续的。这也是为什么目前微商中的代购、面膜之类的东西不能长远的原因。从这点来看当前几个比较热的互联网公司，能得到一定的启示。拨开热闹的表象，观察其是否能照成流程或生产力的提高。比如小米最近的装修，就是一个极好的商业切入点，非常值得资本进入烧钱。同样的还有快的和滴滴。</p>

<p>最后，互联网模式有个重要的特点，即scalable—即高度可复制性，这点下次再讨论。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/04/18/kaggle-bike/">Kaggle入门总结</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-04-18T20:38:00+08:00" pubdate data-updated="true">Apr 18<span>th</span>, 2015</time>
        
         | <a href="/blog/2015/04/18/kaggle-bike/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在知乎上看过一个答案，大意是有个地方叫kaggle，推荐搞机器学习的同学多上去撸一撸，实践出真知。同时还建议先把101系列的题目撸完，再选个感兴趣的比赛做。该答案详情见参考文献。</p>

<h2 id="section">工具</h2>
<p>有工程背景的同学，建议python，拥有不输给R的生态。主要用到以下工具：</p>

<pre><code>ipython notebook + pandas + sklearn
</code></pre>

<p>在面对特别大的数据集，使用了公司的spark。</p>

<ul>
  <li>ipython notebook，神器，请参考我的另一篇blog <a href="http://www.wujiame.com/blog/2014/11/23/ipython-notebook-bring-to-me/">Ipython Notebook对机器学习工程师的价值</a></li>
  <li>pandas: 从工程过来的同学，首先请放弃循环之类的代码实现方式，拥抱dataframe。</li>
  <li>sklearn：在github上非常活跃的项目，请多读官方文档。</li>
  <li>spark：一般kaggle上比赛的数据量级是没有必要用它，但是最近有个比赛train的数据上百g了，所以试了下它。</li>
</ul>

<h2 id="section-1">比赛选择</h2>

<p>首先，请从101系列中选几个做做，该系列一般有详细的教程，熟悉kaggle。接着选几个正在进行的比赛练手。一开始别贪心，注意下数据集的大小，当数据集大于几个g后，工程相关的工作会增加很多，同时对单机的性能有一定的要求，不利于初学者。但是，数据量大更符合真实的情况，比如做过一个ctr预估的比赛，无论是特征工程和模型训练都要更小心谨慎，每次试错的成本很高，随便训练一个模型都需要3-4小时，相应的这个比赛让我意思到sample的重要性，以及一个非常重要的特征处理方法<a href="http://www.wujiame.com/blog/2015/02/10/feature-hashing/">featrue hashing</a>.</p>

<p>本文将重点总结我在做自行车出租数量预测这个比赛的情况。该比赛介绍如下：</p>

<blockquote>
  <p>You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.</p>
</blockquote>

<p>该比赛的好处是符合kaggle的特点。</p>

<ol>
  <li>feature engineering是第一要素。</li>
  <li>ensemble大法好。bagging、averaging一起上，比如到最后我喜欢用random forest + gbdt组合再搞一把，总能给我惊喜。</li>
  <li>时刻小心过拟合。</li>
</ol>

<h2 id="section-2">特征工程</h2>

<ol>
  <li>异常值处理，nan，outlier等。hist和box是两个常用的图形工具。</li>
  <li>数据分布倾斜。 log变化、正负样本重新抽样等。</li>
  <li>特征交叉组合 </li>
  <li>pca或random forest的特征重要性选择。</li>
  <li>特征之间的相关系数。</li>
  <li>特征onehotencoding</li>
</ol>

<h2 id="section-3">模型选择</h2>

<ol>
  <li>Logistic regression</li>
  <li>Random Forest </li>
  <li>gbdt和gbrt</li>
  <li>Factorization Machines</li>
</ol>

<p>LR对特征要求更高，比如很多categorical的特征要作binary编码。我个人倾斜于RF和gbdt，都是属于ensemble思想下的算法。具体RF算法可以参考以前写的一篇blog：<a href="http://www.wujiame.com/blog/2015/02/16/random-forest/">random forest</a></p>

<p>Factorization Machines这个算法好多人用它做ctr预估，后续可以研究下。</p>

<h2 id="section-4">代码分享</h2>

<p>具体代码实现请看我的notebook：
<a href="http://nbviewer.ipython.org/gist/whbzju/ff06fce9fd738dcf8096">kaggle上的自行车出租数量预测</a></p>

<h2 id="section-5">参考文献</h2>

<p>[1] 知乎答案<a href="http://www.zhihu.com/question/23987009">Kaggle如何入门</a></p>

<p>[2] 知乎答案<a href="http://www.zhihu.com/question/24533374">参加kaggle竞赛是怎样一种体验？</a></p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/16/random-forest/">Random Forest</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-02-16T15:09:00+08:00" pubdate data-updated="true">Feb 16<span>th</span>, 2015</time>
        
         | <a href="/blog/2015/02/16/random-forest/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">概述</h2>

<h3 id="section-1">知识背景要求</h3>

<p>本文要求读者对机器学习中的一些基本概念有一定了解，比如特征，交叉验证，generation等概念。随机森林基于决策树模型，读者事先最好对决策树有一定的了解，若完全不了解，请将文中的tree抽象成能告诉你对错的一个black box，则不会影响理解。</p>

<h3 id="section-2">目录</h3>

<ol>
  <li>基本思想</li>
  <li>理论保证</li>
  <li>实践中常用的特性</li>
  <li>实践效果验证</li>
  <li>需要重点注意的</li>
  <li>参考</li>
</ol>

<h2 id="section-3">基本思想</h2>

<h3 id="ensemble-method">Ensemble method</h3>

<p>ensemble是当前主流机器学习领域一个非常流行的概念。引用sklearn的文档：</p>

<blockquote>
  <p>The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</p>
</blockquote>

<p>其又分为两大类：averaging和boosting，分别以Random Forest和AdaBoost算法为代表。</p>

<h3 id="random-forest">Random Forest</h3>

<p>引用wiki的定义：</p>

<blockquote>
  <p>Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. </p>
</blockquote>

<p>直白来讲，随机森林就是将一堆决策树聚在一起，形成一个森林，让每个决策树对测试目标投票，最后对结果求平均，得到最终结果。举个例子：今天你要预测中国队能不能进入世界杯，你找了一百个朋友，让他们每个人给个预测，最后对所有人的预测结果求平均，得到最后中国队能不能出线的结论。直观来理解，这个模型最终结果受到两个因素影响，这一百个朋友中每个人对足球的了解程度和这些人之间的差异程度。</p>

<p>换个正式点的描述：首先，我们假设你了解如果构造一个决策树，随机森林构造了很多个决策树，对于一个数据集N，每次随机抽取n个数据和m个特征，构造一个完全生长的树（不用剪枝），将数据放回重复上述过程，直到生成M棵树。其中有放回的抽样称为bootstrap，是非常重要的概念，下文中让随机森林不用交叉验证的out-of-bag理论基于此。在最初的那篇论文，提到该模型的error受两个因素影响：</p>

<pre><code>* 每个树之间的correction。correction增加，error增加。
* 单独一颗树的strength（不知道怎么翻译，能力？）。strength增加，error下降。
</code></pre>

<p>随机选取m个特征是个关键的步骤，假设总特征有M个，m越小，correction和strength越小。m越大，correction和strength越大。一个合理的m很重要。</p>

<h2 id="section-4">理论保证</h2>

<p>写公式好累，大家去看参考文献吧。</p>

<h2 id="section-5">常用特性</h2>

<ul>
  <li>oob </li>
  <li>feature importance</li>
  <li>聚类</li>
</ul>

<h3 id="the-out-of-bag-oob-error-estimate">The out-of-bag (oob) error estimate</h3>

<p>前面提到过boostrap，这个抽样机制从理论上决定了每次抽样有近三分之一的数据不会被抽到，即可以直接拿来做为测试集，使random forest免去cross validation的过程，节省了时间，称为oob。</p>

<h3 id="feature-importance">feature importance</h3>

<p>random forest是个black box，feature importance特性有助于模型的可解释性。简单考虑下，就算在解释性很强的决策树模型中，如果树过于庞大，人类也很难解释它做出的结果。随机森林通常会有上百棵树组成，更加难以解释。好在我们可以找到那些特征是更加重要的，从而辅助我们解释模型。更加重要的是可以剔除一些不重要的特征，降低杂讯。比起pca降维后的结果，更具有人类的可理解性。</p>

<p>feature importance有好几种方案实现，最常用的是基于一个思想：如果该特征非常的重要，那么稍微改变一点它的值，就会对模型造成很大的影响。再偷个懒，自己造数据太麻烦，可以直接在数据集对该维度的特征数据进行打乱，重新训练测试，打乱前的准确率减去打乱后的准确率就是该特征的重要度。该方法又叫permute。</p>

<h3 id="unsupervised-learning-">Unsupervised learning 聚类</h3>

<p>有点出乎意料，原来随机森林还可以做聚类。总所周知，聚类算法的关键是相识度计算，而随机森林能很好的做相识度计算。首先让我们了解下Proximities：</p>

<blockquote>
  <p>The proximities originally formed a NxN matrix. After a tree is grown, put all of the data, both training and oob, down the tree. If cases k and n are in the same terminal node increase their proximity by one. At the end, normalize the proximities by dividing by the number of trees.</p>
</blockquote>

<p>大致的意思是说，在随机森林模型生成时，统计case k和n在不在同一个叶子节点中，在就加1，最后在安装树的个数归一化。得到case之间的相似度。</p>

<h2 id="section-6">实践效果验证</h2>

<p>在工作中，随机森林被我用来做的最多的事情还是feature importance，验证自己的特征工程效果。另外，由于它的鲁棒性特别好，模型效果在各大模型中排中等，而且对数据预处理要求低，我经常会优先选择用它跑个benchmark。</p>

<p>业余时间还会玩玩kaggle比赛，它就用的比较多，其实gbdt应该更适合比赛，但我个人还是偏好RF。下面放个kaggle的比赛总结，关于如何用它：<a href="https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-random-forests">Getting Started with Random Forests: Titanic Competition</a></p>

<h2 id="section-7">需要重点注意的</h2>

<p>以sklearn的RandomForestClassifier为例，有几个参数需要调下。</p>

<ol>
  <li>n_estimators。决定模型有几颗树，我在做Click-Through Rate Prediction比赛时，将它从default 10改到100，模型效果提升了很多。</li>
  <li>criterion。默认有gini和entropy可选，到底哪个好现在还是有争论的。</li>
  <li>max_features，这个和n_estimators会一起影响整体结果，上文已经提到，可以参数换几个参数试试。</li>
</ol>

<h2 id="section-8">下一步计划</h2>

<p>研究下gradient boosting decision tree</p>

<h2 id="section-9">参考</h2>

<p><a href="http://en.wikipedia.org/wiki/Random_forest">1 wiki RF</a></p>

<p><a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#inter">2 Random Forests Leo Breiman and Adele Cutler</a></p>

<p><a href="http://scikit-learn.org/stable/modules/ensemble.html">3 Ensemble methods</a></p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/10/feature-hashing/">机器学习技巧之feature_hashing</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-02-10T15:58:00+08:00" pubdate data-updated="true">Feb 10<span>th</span>, 2015</time>
        
         | <a href="/blog/2015/02/10/feature-hashing/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">问题</h2>

<p>最近在玩kaggle上的ctr比赛，其训练数据含大量categorical，无法直接用LR模型。举个例子，某个categorical数据集含[苹果，西瓜，梨，桃子]四个类别，一般的处理方法是将这些类别映射成[0,1,2,3]，放入模型中训练。其实这是不合理的，在categorical中，桃子和西瓜并不存在等级差，而变成[1,3]后会存在3&gt;1的问题。以Logistic Regression为代表的算法就无法对该特征学到合适的参数。因此，业界一般会对categorical数据集做onehotencoding，即向量化，还是以上面数据为例子，苹果对应的向量为[1,0,0,0]，桃子对应的为[0,0,0,1]。在sklearn中，可以通过OneHotEncoding或get_dummies实现。显而易见，数据会变得非常稀疏。同时，当categorical的类别变多，特征维度随之剧增，带来的内存存储问题。比如在这次的ctr中，如果采用OneHotEncoding，我60g内存的机器也会报Memory error。</p>

<p>再次，ctr领域或者说高维大数据领域，数据集或多或少的存在稀疏问题。主流ML库都会实现一套稀疏矩阵，应对该问题。feature hashing又称feature trick，类似于kernel trick，在ML领域得到广泛应用的技巧。
维基上的定义：</p>

<blockquote>
  <p>In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array</p>
</blockquote>

<h2 id="section-1">解决方案</h2>

<p>维基上关于Feature Hash的描述非常清晰，各位自己去看，不再累赘，这里多说一点hash的方法。常见的有以下两种实现：</p>

<pre><code>function hashing_vectorizer(features : array of string, N : integer):
 x := new vector[N]
 for f in features:
     h := hash(f)
     x[h mod N] += 1
 return x
</code></pre>

<p>另外还有一种：</p>

<pre><code>function hashing_vectorizer(features : array of string, N : integer):
 x := new vector[N]
 for f in features:
     h := hash(f)
     idx := h mod N
     if ξ(f) == 1:
         x[idx] += 1
     else:
         x[idx] -= 1
 return x
</code></pre>

<p>可以理解，既然是hash，必然要付出collision时的代价。实现方案一并没有考虑处理冲突，N越长，冲突的概率越低，然后存储的要求会变大。实现二，通过有符号的hash来解决冲突问题，即有很大概率在出现冲突时，该hash值为0，即不起作用，更详细的描述参考文献2.</p>

<h2 id="sklearn-featurehasher">sklearn FeatureHasher的实现</h2>

<p><code>class sklearn.feature_extraction.FeatureHasher(n_features=1048576, input_type='dict', dtype=&lt;type 'numpy.float64'&gt;, non_negative=False)</code>，该接口返回一个sparse类型的array。</p>

<blockquote>
  <p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
</blockquote>

<p>该接口需要注意的是数据入参，支持三种格式：pair、dict和string。可以参考官方的<a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/tests/test_feature_hasher.py">featurehasher test</a>。</p>

<p>stackoverflow上也有一个比较好的例子：</p>

<p>Q:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class=""><span class="line">I am using FeatureHasher in scikit-learn.
</span><span class="line">
</span><span class="line">Can anyone explain why I end up with 4 non zero data in the sparse matrix instead of 2 after the following:
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; f = FeatureHasher(input_type='string')
</span><span class="line">&gt;&gt;&gt; g = f.transform(('as','bs'))
</span><span class="line">&lt;2x1048576 sparse matrix of type '&lt;type 'numpy.float64'&gt;'
</span><span class="line">with 4 stored elements in Compressed Sparse Row format&gt;
</span><span class="line">&gt;&gt;&gt; g=f.transform(('as','bs'))
</span><span class="line">&gt;&gt;&gt; g.data
</span><span class="line">array([-1.,  1., -1., -1.])
</span><span class="line">&gt;&gt;&gt; g.nonzero()
</span><span class="line">(array([0, 0, 1, 1], dtype=int32), array([341263, 354738,  98813, 341263], dtype=int32))
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>A:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class=""><span class="line">It appears is expecting a sequence of sequences. The outer sequence being for the observations, and the inner being features. With your input, the inner sequence are the characters of the string.
</span><span class="line">
</span><span class="line">Observation 0: 'a' -&gt; 354738, 's' -&gt; 341263
</span><span class="line">
</span><span class="line">Observation 1: 'b' -&gt; 98813, 's' -&gt; 341263
</span><span class="line">
</span><span class="line">Try this:
</span><span class="line">
</span><span class="line">g = f.transform([['as'],['bs']])
</span><span class="line">For output:
</span><span class="line">
</span><span class="line">&gt;&gt;&gt; g.nonzero()
</span><span class="line">(array([0, 1], dtype=int32), array([494108, 335425], dtype=int32))</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="section-2">参考文献</h2>

<p>[1] <a href="http://en.wikipedia.org/wiki/Feature_hashing">Feature hashing From Wikipedia, the free encyclopedia</a></p>

<p>[2] <a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.</a></p>

<p>[3] <a href="http://scikit-learn.org/stable/modules/feature_extraction.html">sklearn feature hashing</a></p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/02/07/git-practices/">Git 实践</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-02-07T08:54:00+08:00" pubdate data-updated="true">Feb 7<span>th</span>, 2015</time>
        
         | <a href="/blog/2015/02/07/git-practices/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="git-rebasegit-branch">git rebase和git branch</h2>
<p>svn过来的同学一定会觉得git的分支管理好方便，但更应该了解的是git rebase。可以说，用不用git rebase是区分你熟不熟悉git的重要方式。</p>

<h2 id="git-reset">git reset</h2>

<h2 id="git-fetchgit-pull">git fetch和git pull</h2>

<h2 id="git-commit--amend">git commit -amend</h2>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/01/02/2014-readlist/">2014读过的书和参加的公开课</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-01-02T10:45:00+08:00" pubdate data-updated="true">Jan 2<span>nd</span>, 2015</time>
        
         | <a href="/blog/2015/01/02/2014-readlist/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">技术类</h2>

<h3 id="section-1">机器学习</h3>

<ol>
  <li>Pattern Recognizition and Machine Learning. 目前完成前四章，还没有完全吸收，希望2015的成长能够顺利吸收这本书的知识。</li>
  <li>推荐系统cookbook。感觉这本书有些落后于时代，大致翻了下。</li>
  <li>推荐系统实践–项亮。入门好读物。</li>
  <li>Frontiers in Massive Data Analysis。综述型，推荐。</li>
  <li>The Elements of Statistical Learning : Data Mining, Inference, andPrediction。当做工具书，还没有读多少，先把PRML搞定。</li>
  <li>kaggle solution分享。</li>
</ol>

<h3 id="python">Python</h3>

<ol>
  <li>python cookbook. 非常推荐。</li>
  <li>廖雪峰的python<a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000">教程</a>. 简洁概要，实战内容有难度，很适合提供自己编程水平。</li>
  <li>python for data analysis。了解到pandas，数据分析利器。</li>
  <li>sklearn官方文档。图文并茂，极力推荐。</li>
  <li>pandas官方文档。例子丰富，入门先推荐10min那篇。</li>
</ol>

<h3 id="r">R</h3>
<p>粗略看了写文档。</p>

<h2 id="section-2">非技术类</h2>

<ol>
  <li>文明之光（上下），推荐，吴军博士的书质量一如既往。</li>
  <li>女士品茶。概率论发展史及大牛八卦。</li>
  <li>英语语法俱乐部–施元佑。大力推荐，介绍语法的来龙去脉。以前太不看重语法，阅读和写作的瓶颈。</li>
  <li>金字塔原理。</li>
  <li>思考的艺术</li>
  <li>如何阅读一本书</li>
  <li>数理统计简史</li>
</ol>

<h2 id="section-3">公开课</h2>

<ol>
  <li>Andrew Ng的Machine Learing。完成</li>
  <li>台大的机器学习基石。完成</li>
  <li>Functional Programming Principles in Scala。继续上</li>
  <li>Mining Massive Datasets，正在上</li>
  <li>機器學習技法 (Machine Learning Techniques)。正在上</li>
</ol>

<h2 id="section-4">互联网资讯</h2>

<ol>
  <li>咨询：依旧是知乎，目前没有看到什么能替代。</li>
  <li>kaggle，非常好的机器学习学习平台</li>
  <li>cousera，2015能学更多有价值的课程。</li>
</ol>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/12/28/about-ml-fundation-course/">机器学习基石课程总结</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-12-28T11:07:00+08:00" pubdate data-updated="true">Dec 28<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/12/28/about-ml-fundation-course/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>课程一开始，提了四个topic，what every machine learning user should know</p>

<pre><code>* when can ml learn
* why can ml learn
* how can ml learn
* how can ml learn better
</code></pre>

<h2 id="when-can-ml-learn">When can ml learn</h2>
<p>首先，机器学习针对的场景，通过<em>A</em>对<em>D</em>和<em>H</em>学习一个g，用来描述最终的目标f，而这个事情无法简单的用规则搞定。其次，澄清各类细分ml场景的定义：</p>

<pre><code>* 监督式
* 非监督式
* 增强学习
* 推进系统
* Activity学习，通过asking来学习
* Streaming
</code></pre>

<h2 id="why-can-ml-learn">why can ml learn</h2>
<pre><code>* shatter的概念
* break point的概念
* generation问题
* VC维的概念
</code></pre>

<h2 id="how-can-ml-learn">how can ml learn</h2>
<p>讲了一些基本的linear方法，比如logistic regression，顺便提了下nonlinear的问题，通过transform将nonlinear映射到linear可分的空间，有点类似核函数，需要进一步确认。</p>

<h2 id="how-can-ml-learn-better">how can ml learn better</h2>
<pre><code>* overfiting
* regularition，这块数学不错。从拉格朗日的constraint说起，到L1和L2的直观意义。
* cv
* 三个重要的Principle。Occam's Razor， Sample Bias， Data Snooping.
</code></pre>

<h2 id="section">我对这门课的收获</h2>

<pre><code>* 霍夫曼定理
* VC维的理解，线性相关
* 略微有点啰嗦，为了避免数学，导致描述的复杂度上升好几个级别
</code></pre>

<p>总体来讲，对工程人员帮助不是特别大，但有利于加深概念的理解。</p>

<to be="" continue="">
</to>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/30/r-language-conference/">记第七届R语言大会</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-30T11:01:00+08:00" pubdate data-updated="true">Nov 30<span>th</span>, 2014</time>
        
         | <a href="/blog/2014/11/30/r-language-conference/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>这届R语言大会在杭师仓前校区举行，由阿里巴巴承办。邀请到了libsvm的作者林老师。但据同事说林教授演讲用的ppt都是一套（汗~~~）。上午是主会场，有四位嘉宾做了介绍，其他几位介绍的比较范，林稍微带点干货，特别提到数据没有到20T，不一定要上big data。下午是分会场，本人去听了：</p>

<ul>
  <li>分析师使用的R包</li>
  <li>京东的ctr模型</li>
  <li>天猫learning to rank</li>
  <li>天猫数据驱动运营。</li>
</ul>

<h2 id="section">点评</h2>

<ul>
  <li>京东分享推荐的ctr模型。和我街处于同一起跑线，模型和特征做法都很相似。赞下京东的分享着，讲的很实在。</li>
  <li>天猫l2r。没什么干货，介绍了一些基本概念就结束了。</li>
  <li>天猫数据驱动运营。预测销量，c2b。介绍的比较范，而且个人觉得他讲的效果有夸大的嫌疑。</li>
  <li>分析师用的R包。R中类似ipython notebook的东西。可交互的图，非常实用。</li>
</ul>

<h2 id="section-1">感想</h2>

<ul>
  <li>Spark出现的频率很高。</li>
  <li>用R来作云服务，我个人觉得不靠谱。</li>
  <li>libsvm的作者好有趣。演讲前看论文，演讲后睡觉。工业界范十足。</li>
  <li>学术界的分享依旧不靠谱。</li>
  <li>IBM关于电信领域的挖掘是来搞笑的吗</li>
  <li>接下来就等ppt了。</li>
</ul>

<p>最后，和阿里比，其他公司的所处的阶段还是非常的初级。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/23/ipython-notebook-bring-to-me/">Ipython Notebook对机器学习工程师的价值</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-23T16:17:00+08:00" pubdate data-updated="true">Nov 23<span>rd</span>, 2014</time>
        
         | <a href="/blog/2014/11/23/ipython-notebook-bring-to-me/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p><strong>关键词</strong>：代码、数据、文档合一。</p>

<hr />

<p>如果选一个关键词来描述机器学习工程师的工作，不断试错是我心中的number one。相对于软件工程师来讲，有大量琐碎的dirty需要做，通常会占据到80%左右的时间。一个好的工具能够极大的提高效率。</p>

<p>总结需求如下：</p>

<ol>
  <li>可交互式的环境：比如预处理数据，有的时候数据比较大，比较耗时，希望能处理一次后就放在内存里面使用。</li>
  <li>文档化，记录工作流。数据挖掘会有非常多的idea要去尝试，实现这些idea的代码会有微小的差异，需要一个工具能够统一追踪管理他们。且不同的实验会有不同的结果，整理这些结果形成文档太费时间，希望能够做完实验就生成文档。</li>
  <li>经常会有一些片段代码要写，写在文件里有太零碎，写在交互式的shell里面有很难回溯，需要一个交互式和文档结合的工具。</li>
  <li>支持可视化工具，兼容python画图</li>
</ol>

<h2 id="ipython-notebook">神奇的ipython notebook</h2>

<h3 id="section">安装环境</h3>
<p>非常简单，推荐：Anaconda, <a href="https://store.continuum.io/cshop/anaconda/">官网</a></p>

<h3 id="ipython-notebook-1">ipython notebook入门</h3>
<p>还是<a href="http://ipython.org/notebook.html">官网</a>, 一开始不适应的同学，多看几个example吧。</p>

<h3 id="ipython-notebook-2">分享你的ipython notebook</h3>
<p>一键分享：<a href="http://nbviewer.ipython.org/">A simple way to share Jupyter Notebooks</a></p>

<h2 id="section-1">最后</h2>
<p>附一张我的在kaggle上用ipython notebook做的一个入门题照：
<img src="http://wujiarawsrc.qiniudn.com/sklearn-kaggle.png" alt="剧照" /></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/11/22/ml-distance-measure/">聚类算法中常见的距离计算方法</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-11-22T22:01:00+08:00" pubdate data-updated="true">Nov 22<span>nd</span>, 2014</time>
        
         | <a href="/blog/2014/11/22/ml-distance-measure/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="section">概述</h2>
<p>在面对聚类问题时，选择何种距离计算方法求相似度是一个basic question。文献[1]中提到了N多计算方法，从大类来看有以下几种：</p>

<ul>
  <li>$L_p$ Minkowski家族</li>
  <li>$ L_1 $ 家族</li>
  <li>Intersection 家族</li>
  <li>Inner Product家族</li>
  <li>etl
简单算一下大概有40+个计算方法，其中有好多没有听过。好在工业界一般只涉及到几个，本文将按自己理解大致介绍下这些方法及应用情况。</li>
</ul>

<h2 id="section-1">距离的类型和尺度</h2>
<p>类型：</p>

<ul>
  <li>二进制（binary）</li>
  <li>离散值（Discrete）</li>
  <li>连续值(Continuous)</li>
</ul>

<p>尺度：</p>

<ul>
  <li>定性：比如同义：red、green、black，比如顺序：高、中、低</li>
  <li>定量：
  a) interval
  b) ratio
距离的类型和尺度非常重要，影响后续聚类算法的选择。</li>
</ul>

<h2 id="section-2">距离计算方法定义</h2>
<p>严谨的定义参考[4]，通俗来讲，在一个空间内，距离计算方法满足以下4个公理。</p>

<ol>
  <li>$d(x,y) ≥ 0$</li>
  <li>$ d(x,y)=0$ if $x=y$</li>
  <li>$ d(x, y) = d(y, x)$ (distance is symmetric)</li>
  <li>$d(x, y) ≤ d(x, z) + d(z, y)$ (the triangle inequality).</li>
</ol>

<p>在欧式空间，第四个公理可以直观理解为两点之间距离最短。在其他情况需要一些证明才能推导。</p>

<h2 id="section-3">常见的距离计算方法</h2>

<h3 id="lr-norm">Lr norm</h3>
<p>在n维空间，其计算公式如下：</p>

<p>$d(x,y)=(\sum_{k=1}^{n} |x_k-y_k| ^r)^{1/r}$</p>

<ul>
  <li>
    <p>欧式距离
当r=2，这就是我们熟悉的欧式距离，其聚类形状在二维空间是一个圆。归属于$L_2$ norm</p>
  </li>
  <li>
    <p>曼哈顿距离
r=1，归属于$L_1$ norm，其名字的来源与该距离计算过程有关。该距离类似在x和y的每个维度上沿grid line上travel，类似曼哈顿的街道。</p>
  </li>
  <li>
    <p>$L_∞$ norm
当r不断变大，该式中只有max $|x_i-y_i|$项起作用，故又称L_max norm</p>
  </li>
</ul>

<h3 id="jaccard-distance">Jaccard Distance</h3>
<p>类似于相识度，x和y每个维度上相同的值的个数/总的维度。</p>

<p>The comparison of two binary vectors, p and q, leads to four quantities: </p>

<ul>
  <li>M01 = the number of positions where p was 0 and q was 1 </li>
  <li>M10 = the number of positions where p was 1 and q was 0 </li>
  <li>M00 = the number of positions where p was 0 and q was 0 </li>
  <li>M11 = the number of positions where p was 1 and q was 1 </li>
</ul>

<p>The simplest similarity coefficient is the simple matching coefficient </p>

<p>$ J = (M11) / (M01 + M10 + M11) $ </p>

<p>如：</p>

<ul>
  <li>a = 1 0 0 0 0 0 0 0 0 0</li>
  <li>b = 0 0 0 0 0 0 1 0 0 1
J = 0</li>
</ul>

<h3 id="cosine-distance">cosine Distance</h3>
<p>即余弦公式。求x和y两个特征向量的夹角。
cos( d1, d2 ) = (d1 • d2) / ||d1|| ||d2|| </p>

<h3 id="edit-distance">Edit Distance</h3>
<p>一般适用于String point。通过不断的删除和添加单个字符来计算两个string之间的距离。</p>

<h3 id="hamming-distance">Hamming Distance</h3>
<p>pass</p>

<h2 id="section-4">参考</h2>

<ul>
  <li>[1] Comprehensive survey on distance/similarity measures between probability density functions. SH Cha
City, 2007 - csis.pace.edu</li>
  <li>[2] An Introduction to Cluster Analysis for Data Mining</li>
  <li>[3] Unsupervised and Semi-supervised Clustering:
a Brief Survey</li>
  <li>[4] Mining of Massive Datasets - chapter 3</li>
</ul>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>ZJU CS小硕 爱读书 爱代码 相信移动互联网</p>
</section>

<section>
  <h1>新浪微博</h1>
  <ul id="weibo">
    <li>
      <iframe 
        width="100%" 
        height="550" 
        class="share_self" 
        frameborder="0" 
        scrolling="no" 
        src="http://widget.weibo.com/weiboshow/index.php?width=0&height=550&ptype=1&speed=0&skin=&isTitle=0&noborder=1&isWeibo=1&isFans=&uid=1911447995&verifier=4e804b1d">
      </iframe>
    </li>
  </ul>
</section>

<section>
	<h1>Categories</h1>
    	<ul id="categories">
	        <li><a href='/blog/categories/bussiness'>bussiness</a></li><li><a href='/blog/categories/c'>C</a></li><li><a href='/blog/categories/coding'>coding</a></li><li><a href='/blog/categories/git'>git</a></li><li><a href='/blog/categories/jni'>JNI</a></li><li><a href='/blog/categories/learning'>learning</a></li><li><a href='/blog/categories/mac'>Mac</a></li><li><a href='/blog/categories/machine'>machine</a></li><li><a href='/blog/categories/maching'>maching</a></li><li><a href='/blog/categories/octopress'>octopress</a></li><li><a href='/blog/categories/os'>OS</a></li><li><a href='/blog/categories/thoughts'>Thoughts</a></li><li><a href='/blog/categories/tryos'>TryOS</a></li><li><a href='/blog/categories/vim'>vim</a></li><li><a href='/blog/categories/公开课'>公开课</a></li><li><a href='/blog/categories/计步器'>计步器</a></li><li><a href='/blog/categories/读书'>读书</a></li>
	</ul>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/04/18/think-about-weidian-bussiness/">微店引发的互联网模式思考</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/04/18/kaggle-bike/">Kaggle入门总结</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/16/random-forest/">random forest</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/10/feature-hashing/">机器学习技巧之feature_hashing</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/07/git-practices/">git 实践</a>
      </li>
    
  </ul>
</section>






  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

<p>
  Copyright &copy; 2015 - 阿波 -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'hbwu';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>











</body>
</html>
